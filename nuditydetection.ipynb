{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10661309,"sourceType":"datasetVersion","datasetId":6602396},{"sourceId":10661319,"sourceType":"datasetVersion","datasetId":6602404}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-04T14:00:04.766687Z","iopub.execute_input":"2025-02-04T14:00:04.766941Z","iopub.status.idle":"2025-02-04T14:00:18.111007Z","shell.execute_reply.started":"2025-02-04T14:00:04.766915Z","shell.execute_reply":"2025-02-04T14:00:18.110342Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# -------------------------------\n# 1. Read CSV and Prepare the Data\n# -------------------------------\n# CSV file path on Kaggle\ncsv_path = '/kaggle/input/offensivelabels/Dataset.csv'\ndf = pd.read_csv(csv_path)\n\n# Create a new column 'category' based on the CSV labels:\n# If 'nude' column is 1, then label as 'nude', otherwise label as 'safe'.\ndf['category'] = df.apply(lambda row: 'nude' if row['nude'] == 1 else 'safe', axis=1)\n\n# Create a 'filepath' column to indicate the relative path from the image directory.\n# For example, if category is 'nude', then filepath will be 'nude/imagename.jpg'.\ndf['filepath'] = df['category'] + '/' + df['image_name']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T14:00:18.111787Z","iopub.execute_input":"2025-02-04T14:00:18.112357Z","iopub.status.idle":"2025-02-04T14:00:18.144476Z","shell.execute_reply.started":"2025-02-04T14:00:18.112327Z","shell.execute_reply":"2025-02-04T14:00:18.143840Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# -------------------------------\n# 2. Split and Oversample the Data\n# -------------------------------\n# Split the data into training and validation sets (using stratification to preserve class distribution)\ntrain_df, val_df = train_test_split(df, test_size=0.2, stratify=df['category'], random_state=42)\n\n# Separate training data by category\ntrain_df_nude = train_df[train_df['category'] == 'nude']\ntrain_df_safe = train_df[train_df['category'] == 'safe']\n\n# Oversample the minority class (nude) to match the number of safe images.\ntrain_df_nude_oversampled = resample(train_df_nude,\n                                     replace=True,\n                                     n_samples=len(train_df_safe),\n                                     random_state=42)\n\n# Combine the oversampled nude images with the safe images\ntrain_df_balanced = pd.concat([train_df_safe, train_df_nude_oversampled])\n\nprint(\"After oversampling, class distribution:\")\nprint(train_df_balanced['category'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T14:00:32.608599Z","iopub.execute_input":"2025-02-04T14:00:32.608914Z","iopub.status.idle":"2025-02-04T14:00:32.643983Z","shell.execute_reply.started":"2025-02-04T14:00:32.608885Z","shell.execute_reply":"2025-02-04T14:00:32.643093Z"}},"outputs":[{"name":"stdout","text":"After oversampling, class distribution:\ncategory\nsafe    837\nnude    837\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# -------------------------------\n# 3. Set Up Data Generators\n# -------------------------------\nIMG_HEIGHT, IMG_WIDTH = 224, 224\nBATCH_SIZE = 32\n\n# Define a data augmentation generator for the training set.\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n    horizontal_flip=True,\n    rotation_range=20,\n    zoom_range=0.2\n)\n\n# For the validation set, only preprocessing is applied.\nval_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T14:00:44.871641Z","iopub.execute_input":"2025-02-04T14:00:44.871930Z","iopub.status.idle":"2025-02-04T14:00:44.876098Z","shell.execute_reply.started":"2025-02-04T14:00:44.871908Z","shell.execute_reply":"2025-02-04T14:00:44.875198Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Directory where the images are stored (contains subfolders 'nude' and 'safe')\nimages_dir = '/kaggle/input/offensiveimg/dataset'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T14:00:54.302833Z","iopub.execute_input":"2025-02-04T14:00:54.303137Z","iopub.status.idle":"2025-02-04T14:00:54.306698Z","shell.execute_reply.started":"2025-02-04T14:00:54.303112Z","shell.execute_reply":"2025-02-04T14:00:54.305672Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Create the training generator from the balanced training DataFrame.\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=train_df_balanced,\n    directory=images_dir,  # base directory with subfolders\n    x_col='filepath',\n    y_col='category',\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',  # since we have two classes (nude and safe)\n    shuffle=True\n)\n\n# Create the validation generator.\nvalidation_generator = val_datagen.flow_from_dataframe(\n    dataframe=val_df,\n    directory=images_dir,\n    x_col='filepath',\n    y_col='category',\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T14:01:03.385902Z","iopub.execute_input":"2025-02-04T14:01:03.386207Z","iopub.status.idle":"2025-02-04T14:01:04.796142Z","shell.execute_reply.started":"2025-02-04T14:01:03.386183Z","shell.execute_reply":"2025-02-04T14:01:04.795317Z"}},"outputs":[{"name":"stdout","text":"Found 1362 validated image filenames belonging to 2 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py:920: UserWarning: Found 312 invalid image filename(s) in x_col=\"filepath\". These filename(s) will be ignored.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Found 260 validated image filenames belonging to 2 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py:920: UserWarning: Found 33 invalid image filename(s) in x_col=\"filepath\". These filename(s) will be ignored.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# -------------------------------\n# 4. Build the Model Using Transfer Learning\n# -------------------------------\n# Load MobileNetV2 with pretrained ImageNet weights (without the top layer)\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\nbase_model.trainable = False  # Freeze the base model\n\n# Add custom layers for classification\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128, activation='relu')(x)\npredictions = Dense(2, activation='softmax')(x)  # Two classes: nude and safe\n\n# Define the full model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=1e-4),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T14:01:19.327699Z","iopub.execute_input":"2025-02-04T14:01:19.328015Z","iopub.status.idle":"2025-02-04T14:01:24.448540Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m164,226\u001b[0m (641.51 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,226</span> (641.51 KB)\n</pre>\n"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# 5. Train the Model\n# -------------------------------\n# Define callbacks for saving the best model and early stopping\ncallbacks = [\n    ModelCheckpoint('model_best.keras', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1),\n    EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True, verbose=1)\n]\n\nEPOCHS = 20\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples // BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=callbacks\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T14:01:40.831057Z","iopub.execute_input":"2025-02-04T14:01:40.831379Z","iopub.status.idle":"2025-02-04T14:05:51.416917Z","shell.execute_reply.started":"2025-02-04T14:01:40.831354Z","shell.execute_reply":"2025-02-04T14:05:51.416188Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m24/42\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.6147 - loss: 0.6976","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6991 - loss: 0.5764","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (101756928 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: val_accuracy improved from -inf to 0.96484, saving model to model_best.keras\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - accuracy: 0.7025 - loss: 0.5713 - val_accuracy: 0.9648 - val_loss: 0.1269\nEpoch 2/20\n\u001b[1m 1/42\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0504","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2: val_accuracy improved from 0.96484 to 1.00000, saving model to model_best.keras\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0504 - val_accuracy: 1.0000 - val_loss: 0.1020\nEpoch 3/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958ms/step - accuracy: 0.9847 - loss: 0.0837\nEpoch 3: val_accuracy did not improve from 1.00000\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - accuracy: 0.9846 - loss: 0.0837 - val_accuracy: 0.9766 - val_loss: 0.0775\nEpoch 4/20\n\u001b[1m 1/42\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9688 - loss: 0.0806\nEpoch 4: val_accuracy did not improve from 1.00000\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9688 - loss: 0.0806 - val_accuracy: 1.0000 - val_loss: 0.1124\nEpoch 5/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912ms/step - accuracy: 0.9811 - loss: 0.0670\nEpoch 5: val_accuracy did not improve from 1.00000\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1s/step - accuracy: 0.9812 - loss: 0.0668 - val_accuracy: 0.9570 - val_loss: 0.0756\nEpoch 6/20\n\u001b[1m 1/42\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0526\nEpoch 6: val_accuracy did not improve from 1.00000\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0526 - val_accuracy: 1.0000 - val_loss: 0.0851\nEpoch 7/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881ms/step - accuracy: 0.9812 - loss: 0.0517\nEpoch 7: val_accuracy did not improve from 1.00000\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - accuracy: 0.9814 - loss: 0.0515 - val_accuracy: 0.9805 - val_loss: 0.0585\nEpoch 7: early stopping\nRestoring model weights from the end of the best epoch: 2.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# -------------------------------\n# 6. Save and Evaluate the Model\n# -------------------------------\nmodel.save('final_model_oversampled.keras')\n\nval_loss, val_acc = model.evaluate(validation_generator)\nprint(f'Validation accuracy after oversampling: {val_acc:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T14:06:09.652942Z","iopub.execute_input":"2025-02-04T14:06:09.653266Z","iopub.status.idle":"2025-02-04T14:06:18.674319Z","shell.execute_reply.started":"2025-02-04T14:06:09.653241Z","shell.execute_reply":"2025-02-04T14:06:18.673472Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 839ms/step - accuracy: 0.9761 - loss: 0.1240\nValidation accuracy after oversampling: 0.9654\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}